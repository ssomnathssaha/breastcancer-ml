{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import h5py\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from testCases_v2 import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = data['feature_names']\n",
    "features = data['data']\n",
    "label_names = data['target_names']\n",
    "labels = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_labels, test_labels = train_test_split(features, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train.T\n",
    "train_set_y = train_labels.T\n",
    "test_set_x = test.T\n",
    "test_set_y = test_labels.T\n",
    "X_train = np.asarray(train_set_x)\n",
    "Y_train = np.reshape(train_set_y,[455,1])\n",
    "X_test = np.asarray(test_set_x)\n",
    "Y_test = np.reshape(test_set_y,[114,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    #print probas[0,2]\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    acc=0\n",
    "    y=y.T\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if p[0,i]==y[0,i]:\n",
    "            acc=acc+1\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(\"Accuracy:\"+str(acc/y.shape[0])+\"%\")\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.00001, num_iterations = 60, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches =  L_model_forward(X,parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL,Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL,Y,caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 3 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 3 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    p = predict(X_test, Y_test, parameters)\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 30.843251\n",
      "Cost after iteration 3: 29.011867\n",
      "Cost after iteration 6: 27.229002\n",
      "Cost after iteration 9: 25.491777\n",
      "Cost after iteration 12: 23.797487\n",
      "Cost after iteration 15: 22.143063\n",
      "Cost after iteration 18: 20.527253\n",
      "Cost after iteration 21: 18.947276\n",
      "Cost after iteration 24: 17.403913\n",
      "Cost after iteration 27: 15.897170\n",
      "Cost after iteration 30: 14.434196\n",
      "Cost after iteration 33: 12.995999\n",
      "Cost after iteration 36: 11.536717\n",
      "Cost after iteration 39: 10.397698\n",
      "Cost after iteration 42: 9.112654\n",
      "Cost after iteration 45: 7.414758\n",
      "Cost after iteration 48: 5.652759\n",
      "Cost after iteration 51: 3.902941\n",
      "Cost after iteration 54: 2.255248\n",
      "Cost after iteration 57: 1.123876\n",
      "Cost after iteration 60: 0.711841\n",
      "Accuracy:95%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niUBEJMYpEYiIxDglAhGRGKdEICIS4/4f+JsvAbFszOEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1039d130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_dims = [30,20,15,10,1]\n",
    "parameters = L_layer_model(X_train, Y_train.T, layers_dims, num_iterations = 63, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

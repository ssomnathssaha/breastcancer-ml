{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 30.843251\n",
      "Cost after iteration 3: nan\n",
      "Cost after iteration 6: nan\n",
      "Cost after iteration 9: nan\n",
      "Cost after iteration 12: nan\n",
      "Cost after iteration 15: nan\n",
      "Cost after iteration 18: nan\n",
      "Cost after iteration 21: nan\n",
      "Cost after iteration 24: nan\n",
      "Cost after iteration 27: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:271: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:353: RuntimeWarning: divide by zero encountered in divide\n",
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:353: RuntimeWarning: invalid value encountered in divide\n",
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:105: RuntimeWarning: invalid value encountered in multiply\n",
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:60: RuntimeWarning: invalid value encountered in maximum\n",
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in less_equal\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6595b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import h5py\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from testCases_v2 import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "feature_names = data['feature_names']\n",
    "features = data['data']\n",
    "label_names = data['target_names']\n",
    "labels = data['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test, train_labels, test_labels = train_test_split(features, labels, test_size=0.20, random_state=42)\n",
    "\n",
    "train_set_x = train.T\n",
    "train_set_y = train_labels.T\n",
    "test_set_x = test.T\n",
    "test_set_y = test_labels.T\n",
    "\n",
    "import numpy as np\n",
    "X_train = np.asarray(train_set_x)\n",
    "Y_train = np.reshape(train_set_y,[455,1])\n",
    "X_test = np.asarray(test_set_x)\n",
    "Y_test = np.reshape(test_set_y,[114,1])\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters     \n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.075, num_iterations = 30, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches =  L_model_forward(X,parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL,Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL,Y,caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 3 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 3 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "layers_dims = [30, 20, 15, 10, 1]\n",
    "parameters = L_layer_model(X_train, Y_train.T, layers_dims, num_iterations = 30, print_cost = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
